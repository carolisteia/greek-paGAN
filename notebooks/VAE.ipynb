{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoQEjIKEmnDq",
        "outputId": "2526d412-d6fc-4be5-b35c-6b93523c63aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages if not already installed\n",
        "# !pip install tensorflow numpy matplotlib\n",
        "\n",
        "# Importing required libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten, BatchNormalization, LeakyReLU, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Input, Lambda\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our project, we used Google Collab. So, if you're using Google Colab, mount Google Drive and change directory accordingly."
      ],
      "metadata": {
        "id": "VR98Ck4f1ZYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change directory to where your images are stored\n",
        "os.chdir('/content/drive/My Drive/crops_dataset_trie/epsilon')"
      ],
      "metadata": {
        "id": "h5ca1ub_2Asm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load images, preprocess them (resize, normalize), and display an example."
      ],
      "metadata": {
        "id": "TZeGSlWE2EAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess images\n",
        "folder_path = '/content/drive/My Drive/crops_dataset_trie/epsilon'\n",
        "file_list = os.listdir(folder_path)\n",
        "\n",
        "#It browses all the files in the specified folder (folder_path). For each file ending in .jpg or .png, it loads the image, converts it to RGB format, resizes it to 64x64 pixels and then adds it to the images list.\n",
        "images = []\n",
        "for file in file_list:\n",
        "    if file.endswith('.jpg') or file.endswith('.png'):\n",
        "        img_path = os.path.join(folder_path, file)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (64, 64))\n",
        "        images.append(img)\n",
        "\n",
        "# Display an example image :uses Matplotlib to display the pre-processed image at index 0 of the images list to get an idea of the result of pre-processing the images\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(images[0])\n",
        "plt.axis('off')\n",
        "plt.title('Example of letter E')\n",
        "plt.show()\n",
        "\n",
        "# It concerts images to numpy array and normalize :converts the image list, which now contains pre-processed images, into a NumPy array to be compatible with numerical calculation operations. It then normalises the pixel values by dividing them by 255.0, bringing them into the range [0, 1]. This manipulation is very common in image processing for machine learning.\n",
        "images = np.array(images)\n",
        "images = images.astype('float32') / 255.0\n"
      ],
      "metadata": {
        "id": "obpkLH3h2Hny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have defined the VAE model, including encoder, decoder, and loss function."
      ],
      "metadata": {
        "id": "S1FgBdGO2OC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dimensions : we define the dimensions of the input images (input_shape) as 64x64 pixel images with 3 colour channels (RGB). The size of the latent space (latent_dim) is set to 200. This size can be modified to improve the model or according to the data (our sub-directories do not have the same quantity or quality of images).\n",
        "input_shape = (64, 64, 3)\n",
        "latent_dim = 200\n",
        "\n",
        "# Encoder :We create an encoder model that takes the original images as input and transforms them into a flat vector. This vector then passes through two fully connected layers (Dense) with ReLU activations to extract important features.\n",
        "#ReLU (Rectified Linear Unit) is a mathematical function used in neural networks to decide which information is important to retain when processing data. It helps to make neural networks more efficient by amplifying useful signals and filtering out noise. The use of RelU was suggested by an acquaintance of Emilie GUIDI, who wishes to remain anonymous but is studying computer science at the ENS in Lyon.\n",
        "inputs = Input(shape=input_shape)\n",
        "x = Flatten()(inputs)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "\n",
        "# Latent space parameters :These Dense layers take the input data x, which is the output of the encoder, and produce z_mean and z_log_var, which are the mean and logarithm of the variance of the latent distribution respectively.\n",
        "#This allows important characteristics of the data to be represented concisely and probabilistically, facilitating the generation and accurate reconstruction of new data in a Variational Autoencoder (VAE).\n",
        "z_mean = Dense(latent_dim)(x)\n",
        "z_log_var = Dense(latent_dim)(x)\n",
        "\n",
        "# Sampling function : here's a simplistic explanation of how sampling works to get the idea = given a standard normal distribution represented by a centred shape with denser points in the middle, we use two parameters, z_mean and z_log_var. These parameters act to stretch or compress this normal distribution, influencing where the points will be more concentrated or dispersed. Using these parameters, we sample points (z) in latent space.\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.random.normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "# Decoder :In this section, we define how the decoder reconstructs data from latent space in a Variational Autoencoder (VAE). The lines of code show that we take samples of the latent space (decoder_inputs), pass them through several layers of neurons (Dense), activated by ReLU to learn complex representations, then finally use an output layer with sigmoid activation to generate values between 0 and 1, corresponding to each pixel of the reconstructed image, which is then reshaped to match the original input shape (input_shape).\n",
        "#In the context of a Variational Autoencoder (VAE) decoder, we have chosen sigmoid activation to be applied to the last layer of neurons to produce output values in the range 0 to 1. This is particularly appropriate as we are dealing with image data where pixel values are typically normalised between 0 and 1.\n",
        "decoder_inputs = Input(shape=(latent_dim,))\n",
        "x = Dense(256, activation='relu')(decoder_inputs)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(np.prod(input_shape), activation='sigmoid')(x)\n",
        "outputs = Reshape(input_shape)(x)\n",
        "\n",
        "# Models :these lines of code thus define the three important parts of our Variational Autoencoder (VAE) model. As we have seen, the first, called the encoder, takes the input data and transforms it into a simplified representation in what we call a 'latent space'. This captures the main characteristics of the data. Next, the \"decoder\" takes this simplified representation and transforms it again into a reconstructed version of the original data. Finally, the full VAE model uses both the encoder and the decoder to learn how to represent and generate new data from this simplified representation.\n",
        "encoder = Model(inputs, z_mean)\n",
        "decoder = Model(decoder_inputs, outputs)\n",
        "vae_outputs = decoder(z)\n",
        "vae = Model(inputs, vae_outputs)\n",
        "\n",
        "# VAE loss function : To create the vae_loss function used in a Variational Autoencoder (VAE), we asked ChatGPT for help. This function is important because it helps us measure two things: firstly, how accurately the VAE reconstructs the input data, and secondly, how efficiently it organises the data in a latent space to generate new data. By optimising this function during training, the VAE learns to faithfully reproduce the data while maintaining a useful representation of the information.\n",
        "def vae_loss(inputs, outputs):\n",
        "    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n",
        "    reconstruction_loss *= np.prod(input_shape)\n",
        "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "    kl_loss = K.sum(kl_loss, axis=-1)\n",
        "    kl_loss *= -0.5\n",
        "    return K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Adding VAE loss to the model\n",
        "vae.add_loss(vae_loss(inputs, vae_outputs))\n",
        "\n",
        "# Compile VAE model : This line of code compiles the Variational Autoencoder (VAE) model for the training phase by specifying the optimizer to be used, in this case Adam(). Compiling the model means preparing the neural network for training by defining how it should update itself when it sees data, in particular by which optimisation algorithm it should adjust its weights and biases. Adam is a common choice of optimizer in deep learning, which is why we chose it.\n",
        "vae.compile(optimizer=Adam())\n"
      ],
      "metadata": {
        "id": "H1Rq-g5y2U-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the VAE model and generate images using the trained decoder !"
      ],
      "metadata": {
        "id": "klfg5dbc2Z3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training VAE :The VAE is trained on the image data using itself as the target (images) for reconstruction. This is done for 30 epochs (complete iterations on the whole data set) with a batch size of 35, which means that the data is divided into batches of 35 images for training. We will try to modify these hyperparameters to refine our results.\n",
        "vae.fit(images, images, epochs=30, batch_size=35)\n",
        "\n",
        "# Generating random latent vectors :Random latent vectors are generated from a normal distribution with a size of (num_images_to_generate, latent_dim). This creates 10 random latent vectors, each with a latent_dim dimension, which is the size of the latent space learned by the VAE.\n",
        "num_images_to_generate = 10\n",
        "latent_vectors = np.random.normal(size=(num_images_to_generate, latent_dim))\n",
        "\n",
        "# Generate images using decoder : The decoder uses the latent vectors generated to reconstruct images from the latent space. Each latent vector is transformed into an image by the decoder.\n",
        "generated_images = decoder.predict(latent_vectors)\n",
        "\n",
        "# Display generated images :The images generated are displayed in a grid for viewing. Each image corresponds to a reconstruction generated by the decoder from random latent vectors. The x and y axis is deactivated (plt.axis(\"off\")) to show only the images themselves, with a numbered title for each image.\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(num_images_to_generate):\n",
        "    ax = plt.subplot(2, num_images_to_generate // 2, i + 1)\n",
        "    plt.imshow(generated_images[i])\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Image {i + 1}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SXFyEwK82gIM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}